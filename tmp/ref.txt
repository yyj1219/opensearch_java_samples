import org.opensearch.client.json.JsonData;
import org.opensearch.client.opensearch.OpenSearchClient;
import org.opensearch.client.opensearch._types.*;
import org.opensearch.client.opensearch._types.aggregations.*;
import org.opensearch.client.opensearch._types.query_dsl.*;
import org.opensearch.client.opensearch.core.SearchRequest;
import org.opensearch.client.opensearch.core.SearchResponse;
import org.opensearch.client.opensearch.core.search.Hit;
import org.opensearch.client.opensearch.core.search.SourceConfig;
import scouter.db.elastic.metering.XLogHistoMetering;
import scouter.db.elastic.vo.HeatMapBucket;
import scouter.db.elastic.vo.XLogHistoVo;
import scouter.lang.TextTypes;
import scouter.lang.args.TopTxSearchArgs;
import scouter.lang.args.XLogInfoSearchArgs;
import scouter.lang.counters.E2ETypeConstants;
import scouter.server.Logger;
import scouter.server.util.QueryUtil;
import scouter.util.DateUtil;
import scouter.config.CommonConfigure;
import tuna.server.db.common.TextSearchHelper;
import tuna.server.db.common.TextSearchInfo;
import tuna.server.db.common.opensearch.OpenSearchConnectionManager;
import tuna.server.db.common.opensearch.OpenSearchUtil;
import tuna.server.db.rd.IEndUserTxRD;
import tuna.server.db.rd.factory.TextRDFactory;
import tuna.server.db.rd.opensearch.TextOschRD;
import tuna.server.text.cache.LocalTextCache;
import java.text.NumberFormat;
import java.util.*;
import java.util.stream.Collectors;

public class EndUserTxOschRD implements IEndUserTxRD {
    private final CommonConfigure conf;
    private final int XLOG_HISTO_TIME_BUCKETS = 150;
    private final TextSearchHelper textSearchHelper;

    public EndUserTxOschRD() {
        this.textSearchHelper = TextSearchHelper.getInstance();
        this.conf = CommonConfigure.getInstance();
    }

    //  EndUser Xlog 전체를 가져오는 Method
    //  Scroll 을 이용해 전체 데이터를 가져옴
    //  2019.07.22  최대 1,000 건만 조회하도록 변경된 LoadEndUserXLogInfo 를 사용함
    //  추후 전체데이터를 요청할 경우에 대비하여 Method를 남겨두고 Deprecate 함
    @Override
    public List<Map<String, Object>> LoadEndUserXLogInfo(XLogInfoSearchArgs args) {
        try {
            List<String> timeList = DateUtil.getSearchRangeTime("enduser-info-", args.from, args.to);
            if (timeList == null) return null;

            String[] indexes = timeList.toArray(new String[0]);

            List<Query> filterQueryList = new ArrayList<>();
            List<Query> mustNotQueryList = new ArrayList<>();

            filterQueryList.add(
                    RangeQuery.of(r -> r
                            .field("endTime")
                            .gte(JsonData.of(args.from))
                            .lte(JsonData.of(args.to))
                    ).toQuery()
            );
            filterQueryList.add(
                    RangeQuery.of(r -> r
                            .field("elapsedTime")
                            .gte(JsonData.of(args.fromElapsed))
                            .lte(JsonData.of(args.toElapsed))
                    ).toQuery()
            );
            filterQueryList.add(
                    TermsQuery.of(t -> t
                            .field("objHash")
                            .terms(v -> v.value(OpenSearchUtil.getFieldValueList(args.objList)))
                    ).toQuery()
            );

            if (args.serviceHash != 0L) {
                filterQueryList.add(
                        MatchQuery.of(m -> m
                                .field("serviceHash")
                                .query(FieldValue.of(args.serviceHash))
                        ).toQuery()
                );
            }
            mustNotQueryList.add(
                    MatchQuery.of(m -> m
                            .field("type")
                            .query(FieldValue.of(E2ETypeConstants.ERROR))
                    ).toQuery()
            );

            Map<String, Object> filters = args.fileterMap;
            for (String key : filters.keySet()) {
                if (key.equals("elapsed")) {
                    filterQueryList.add(
                            RangeQuery.of(r -> r
                                    .field("elapsedTime")
                                    .gte(JsonData.of(filters.get(key)))
                            ).toQuery()
                    );
                } else if (key.equals("error")) {
                    mustNotQueryList.add(
                            MatchQuery.of(m -> m
                                    .field("errorHash")
                                    .query(FieldValue.of(0))
                            ).toQuery()
                    );
                } else {
                    TermsQuery.of(t -> t
                            .field(key)
                            .terms(tqf -> tqf.value(OpenSearchUtil.getFieldValueList(filters.get(key))))
                    ).toQuery();
                }
            }

            String order = args.order != null && !args.order.equals("") ? args.order : "elapsedTime";
            SortOrder orderCmd = args.orderCmd != null && args.orderCmd.equals("asc") ? SortOrder.Asc : SortOrder.Desc;

            List<String> docValueFieldList = Arrays.asList(
                    "domProcessingTime", "elapsedTime", "endTime", "errorHash", "networkTime", "objHash",
                    "objName", "serverTime", "serviceHash", "timeToDomComplete", "timeToDomInteracitve",
                    "loadTime", "connectionTime", "sslConnectionTime", "dnsLookupTime", "timeToFirstByteRecv",
                    "type", "userIp", "gxid", "agentHash", "pageProcessingTime", "os", "browser", "uuid");
            ArrayList<FieldAndFormat> docValueFields = new ArrayList<>();
            for (String docValueField : docValueFieldList) {
                docValueFields.add(FieldAndFormat.of(f -> f.field(docValueField)));
            }
            SearchRequest request = new SearchRequest.Builder()
                    .index(Arrays.asList(indexes))
                    .size(conf.es_query_fetch_size)
                    .sort(s -> s.field(f -> f.field(order).order(orderCmd)))
                    .query(Query.of(q -> q.bool(bool -> bool.filter(filterQueryList).mustNot(mustNotQueryList))))
                    .source(SourceConfig.of(sc -> sc.fetch(false)))
                    .docvalueFields(docValueFields)
                    .ignoreUnavailable(true)
                    .allowNoIndices(true)
                    .expandWildcards(ExpandWildcard.Open)
                    .build();

            if (conf.print_es_query) {
                OpenSearchUtil.printJson(request);
            }

            OpenSearchClient client = Objects.requireNonNull(OpenSearchConnectionManager.getInstance()).getReadClient();
            SearchResponse<Map> response = client.search(request, Map.class);
            List<Map<String, Object>> resultList = new ArrayList<>();
            for (Hit<Map> hit : response.hits().hits()) {
                Map<String, Object> valueMap = new HashMap<>();
                for (Map.Entry<String, JsonData> field : Objects.requireNonNull(hit.fields()).entrySet()) {
                    String fieldName = field.getKey();
                    switch (fieldName) {
                        case "endTime":
                            valueMap.put("endTime", Long.parseLong(field.getValue().toString()));
                            break;
                        case "serviceHash":
                            int serviceHash = Integer.parseInt(field.getValue().toString());
                            valueMap.put(fieldName, serviceHash);
                            valueMap.put("service", textSearchHelper.searchText(new TextSearchInfo(), TextTypes.SERVICE, serviceHash));
                            break;
                        case "errorHash":
                            int errorHash = Integer.parseInt(field.getValue().toString());
                            valueMap.put("error", errorHash);
                            valueMap.put("errorMsg", textSearchHelper.searchText(new TextSearchInfo(), TextTypes.ERROR, errorHash));
                            break;
                        default:
                            valueMap.put(fieldName, field.getValue());
                            break;
                    }
                }
                long endTime = Long.parseLong(valueMap.get("endTime").toString());
                int elapsed = Integer.parseInt(valueMap.get("elapsedTime").toString());
                valueMap.put("startTime", endTime - elapsed);
                resultList.add(valueMap);
            }

            LocalTextCache localTextCache = null;
            TextSearchInfo textSearchInfo = new TextSearchInfo();
            if (textSearchInfo.getSize() > 0) {
                localTextCache = TextOschRD.getInstance().getString(textSearchInfo);
                if (localTextCache != null) {
                    for (Map<String, Object> map : resultList) {
                        if (map.get("serviceHash") != null && map.get("service") == null) {
                            map.put("service", localTextCache.get(TextTypes.SERVICE, Integer.parseInt(map.get("serviceHash").toString())));
                        }
                        if (map.get("error") != null && map.get("errorMsg") == null) {
                            map.put("errorMsg", localTextCache.get(TextTypes.ERROR, Integer.parseInt(map.get("error").toString())));
                        }
                    }
                }
            }
            if (conf.print_es_query_result) {
                QueryUtil.print(resultList);
            }
            return resultList;
        } catch (Exception e) {
            Logger.println(e);
            return null;
        }
    }

    public Map<String, Object> LoadEndUserXLogHistogram(XLogInfoSearchArgs args) throws Exception {
        try {
            List<String> timeList = DateUtil.getSearchRangeTime("enduser-info-", args.from, args.to);
            if (timeList == null) return null;

            String[] indexes = timeList.toArray(new String[0]);

            List<Query> filterQueryList = new ArrayList<>();
            List<Query> mustNotQueryList = new ArrayList<>();

            filterQueryList.add(
                    RangeQuery.of(r -> r
                            .field("endTime")
                            .gte(JsonData.of(args.from))
                            .lte(JsonData.of(args.to))
                    ).toQuery()
            );
            filterQueryList.add(
                    TermsQuery.of(t -> t
                            .field("objHash")
                            .terms(v -> v.value(OpenSearchUtil.getFieldValueList(args.objList)))
                    ).toQuery()
            );

            Map<String, Object> filters = args.fileterMap;
            for (String key : filters.keySet()) {
                if (key.equals("elapsed")) {
                    filterQueryList.add(
                            RangeQuery.of(r -> r
                                    .field("elapsedTime")
                                    .gte(JsonData.of(filters.get(key)))
                            ).toQuery()
                    );
                } else if (key.equals("error")) {
                    mustNotQueryList.add(
                            MatchQuery.of(m -> m
                                    .field("errorHash")
                                    .query(FieldValue.of(0))
                            ).toQuery()
                    );
                } else {
                    TermsQuery.of(t -> t
                            .field(key)
                            .terms(tqf -> tqf.value(OpenSearchUtil.getFieldValueList(filters.get(key))))
                    ).toQuery();
                }
            }

            if (args.serviceHash != 0L) {
                filterQueryList.add(
                        MatchQuery.of(m -> m
                                .field("serviceHash")
                                .query(FieldValue.of(args.serviceHash))
                        ).toQuery()
                );
            }
            mustNotQueryList.add(
                    MatchQuery.of(m -> m
                            .field("type")
                            .query(FieldValue.of(E2ETypeConstants.ERROR))
                    ).toQuery()
            );

            long searchInterval = args.to - args.from;
            long histogramTimeInterval = searchInterval / this.XLOG_HISTO_TIME_BUCKETS;
            Logger.println("histogramTimeInterval : " + histogramTimeInterval);

            // set histogram unit : time, elapsed
            List<Map<String, CompositeAggregationSource>> compSources = List.of(
                    Map.of("time",
                            CompositeAggregationSource.of(s -> s
                                    .histogram(h -> h
                                            .field("endTime")
                                            .interval((double) histogramTimeInterval)
                                    )
                            )
                    ),
                    Map.of("elapsed",
                            CompositeAggregationSource.of(s -> s
                                    .histogram(h -> h
                                            .field("elapsedTime")
                                            .interval((double) conf.xlog_hist_elapsed_interval)
                                    )
                            )
                    )
            );

            SearchRequest request = new SearchRequest.Builder()
                    .index(Arrays.asList(indexes))
                    .query(Query.of(q -> q.bool(bool -> bool.filter(filterQueryList).mustNot(mustNotQueryList))))
                    .size(0)
                    .aggregations("aggs", agg -> agg
                            .composite(comp -> comp
                                    .size(conf.es_composite_bucket_size)
                                    .sources(compSources)
                            )
                            .aggregations("errSum", subAgg -> subAgg
                                    .sum(s -> s.field("errorHash"))
                            )
                    )
                    .ignoreUnavailable(true)
                    .allowNoIndices(true)
                    .expandWildcards(ExpandWildcard.Open)
                    .build();

            if (conf.print_es_query) {
                OpenSearchUtil.printJson(request);
            }

            OpenSearchClient client = Objects.requireNonNull(OpenSearchConnectionManager.getInstance()).getReadClient();

            boolean initialSearch = true;
            int bucketSize;
            List<XLogHistoVo> list = new ArrayList<>();
            Map<String, String> afterKey = null;

            do {
                if (!initialSearch) {
                    Map<String, String> finalAfterKey = afterKey;
                    request = new SearchRequest.Builder()
                            .index(Arrays.asList(indexes))
                            .query(Query.of(q -> q.bool(bool -> bool.filter(filterQueryList).mustNot(mustNotQueryList))))
                            .size(0)
                            .aggregations("aggs", agg -> agg
                                    .composite(comp -> comp
                                            .size(conf.es_composite_bucket_size)
                                            .sources(compSources)
                                            .after(finalAfterKey) // 첫 실행이 아니면 request 에 after 추가해서 조회
                                    )
                                    .aggregations("errSum", subAgg -> subAgg
                                            .sum(s -> s.field("errorHash"))
                                    )
                            )
                            .ignoreUnavailable(true)
                            .allowNoIndices(true)
                            .expandWildcards(ExpandWildcard.Open)
                            .build();
                } else {
                    initialSearch = false;
                }

                SearchResponse<Map> response = client.search(request, Map.class);
                CompositeAggregate composite = response.aggregations().get("aggs").composite();
                bucketSize = composite.buckets().array().size();

                for (CompositeBucket bucket : composite.buckets().array()) {
                    XLogHistoVo vo = new XLogHistoVo();
                    vo.time = ((Number) bucket.key().get("time")).longValue();
                    vo.elapsed = ((Number) bucket.key().get("elapsed")).intValue();
                    vo.count = bucket.docCount();
                    vo.error = bucket.aggregations().get("errSum").sum().value();
                    list.add(vo);
                }

                afterKey = composite.afterKey().entrySet().stream()
                        .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().toString()));
            } while (bucketSize > 0);

            if (list.isEmpty()) {
                return Collections.singletonMap("xlog", new ArrayList<>());
            }

            TreeMap<Long, HeatMapBucket[]> result = new XLogHistoMetering().process(list, histogramTimeInterval, args);

            List<Map<String, List<Integer>>> xlogList = new ArrayList<>();
            for (Map.Entry<Long, HeatMapBucket[]> entry : result.entrySet()) {
                Long time = entry.getKey();
                HeatMapBucket[] buckets = entry.getValue();
                List<Integer> innerList = new ArrayList<>();
                for (HeatMapBucket bucket : buckets) {
                    if (bucket.error) {
                        bucket.count = -bucket.count;
                    }
                    innerList.add(bucket.count);
                }
                xlogList.add(Collections.singletonMap(time.toString(), innerList));
            }

            //  RESULT PRINT
            Map<String, Object> resultMap = Collections.singletonMap("xlog", xlogList);
            if (conf.print_es_query_result) {
                resultMap.forEach((key, value) -> Logger.println(key + " : " + value));
            }

            System.out.println("EndUserTxOsch xlogList Size : " + xlogList.size());
            return resultMap;

        } catch (Exception e) {
            System.out.println(e.getMessage());
            throw e;
        }
    }

    @Override
    public List<Map<String, Object>> LoadEndUserTopNTransaction(TopTxSearchArgs args) throws Exception {
        try {
            List<String> timeList = DateUtil.getSearchRangeTime("enduser-info-", args.from, args.to);
            if (timeList == null) return new ArrayList<>();

            String[] indexes = timeList.toArray(new String[0]);

            List<Query> filterQueryList = new ArrayList<>();
            List<Query> mustQueryList = new ArrayList<>();
            List<Query> mustNotQueryList = new ArrayList<>();

            filterQueryList.add(
                    RangeQuery.of(r -> r
                            .field("endTime")
                            .gte(JsonData.of(args.from))
                            .lte(JsonData.of(args.to))
                    ).toQuery()
            );
            filterQueryList.add(
                    TermsQuery.of(t -> t
                            .field("objHash")
                            .terms(v -> v.value(OpenSearchUtil.getFieldValueList(args.objList)))
                    ).toQuery()
            );

            mustQueryList.add(
                    MatchQuery.of(m -> m
                            .field("type")
                            .query(FieldValue.of(E2ETypeConstants.DOCUMENT))
                    ).toQuery()
            );

            Map<String, Object> filters = args.fileterMap;
            for (String key : filters.keySet()) {
                if (key.equals("elapsed")) {
                    filterQueryList.add(
                            RangeQuery.of(r -> r
                                    .field("elapsedTime")
                                    .gte(JsonData.of(filters.get(key)))
                            ).toQuery()
                    );
                } else if (key.equals("error")) {
                    mustNotQueryList.add(
                            MatchQuery.of(m -> m
                                    .field("errorHash")
                                    .query(FieldValue.of(0))
                            ).toQuery()
                    );
                } else {
                    TermsQuery.of(t -> t
                            .field(key)
                            .terms(tqf -> tqf.value(OpenSearchUtil.getFieldValueList(filters.get(key))))
                    ).toQuery();
                }
            }

            if (args.serviceHash != 0L) {
                filterQueryList.add(
                        MatchQuery.of(m -> m
                                .field("serviceHash")
                                .query(FieldValue.of(args.serviceHash))
                        ).toQuery()
                );
            }
            mustNotQueryList.add(
                    MatchQuery.of(m -> m
                            .field("type")
                            .query(FieldValue.of(E2ETypeConstants.ERROR))
                    ).toQuery()
            );

            Aggregation aggregation = Aggregation.of(a -> a
                    .terms(t -> t
                            .field("serviceHash")
                            .size(args.top)
                            .shardSize(args.shard_size > 0 ? args.shard_size : null)
                            .order(Map.of("elapsedTimeStat.avg", SortOrder.Desc))
                    )
                    .aggregations(Map.of(
                            "elapsedTimeStat", Aggregation.of(a1 -> a1.extendedStats(s -> s.field("elapsedTime"))),
                            "timeToFirstByteRecvStat", Aggregation.of(a2 -> a2.extendedStats(s -> s.field("timeToFirstByteRecv"))),
                            "timeToDomCompleteStat", Aggregation.of(a3 -> a3.extendedStats(s -> s.field("timeToDomComplete"))),
                            "call", Aggregation.of(a4 -> a4.valueCount(v -> v.field("serviceHash")))
                    ))
            );

            SearchRequest request = new SearchRequest.Builder()
                    .index(Arrays.asList(indexes))
                    .query(Query.of(q -> q.bool(bool -> bool.filter(filterQueryList).must(mustQueryList).mustNot(mustNotQueryList))))
                    .size(0)
                    .aggregations("topN", aggregation)
                    .build();

            if (conf.print_es_query) {
                OpenSearchUtil.printJson(request);
            }

            OpenSearchClient client = Objects.requireNonNull(OpenSearchConnectionManager.getInstance()).getReadClient();

            SearchResponse<Map> response = client.search(request, Map.class);
            List<Map<String, Object>> result = new ArrayList<>();

            //  조회결과가 없는 경우 Empty List Return
            if (response.aggregations() == null || !response.aggregations().containsKey("topN")) {
                return result;
            }

            LongTermsAggregate lterms = response.aggregations().get("topN").lterms();
            for (LongTermsBucket bucket : lterms.buckets().array()) {
                ExtendedStatsAggregate elapsedTimeStat = bucket.aggregations().get("elapsedTimeStat").extendedStats();
                ExtendedStatsAggregate timeToFirstByteRecvStat = bucket.aggregations().get("timeToFirstByteRecvStat").extendedStats();
                ExtendedStatsAggregate timeToDomCompleteStat = bucket.aggregations().get("timeToDomCompleteStat").extendedStats();

                long hash = Long.valueOf(bucket.key());

                Map<String, Object> innerMap = new HashMap<>();
                innerMap.put("hash", hash);
                innerMap.put("name", TextRDFactory.getTextRD().getString(args.from, args.to, TextTypes.SERVICE, hash));
                innerMap.put("call", bucket.aggregations().get("call").valueCount().value());
                innerMap.put("maxElapsed", elapsedTimeStat.max());
                innerMap.put("minElapsed", elapsedTimeStat.min());
                innerMap.put("avgElapsed", elapsedTimeStat.avg());
                innerMap.put("devElapsed", elapsedTimeStat.stdDeviation());
                innerMap.put("maxFirstByte", timeToFirstByteRecvStat.max());
                innerMap.put("minFirstByte", timeToFirstByteRecvStat.min());
                innerMap.put("avgFirstByte", timeToFirstByteRecvStat.avg());
                innerMap.put("devFirstByte", timeToFirstByteRecvStat.stdDeviation());
                innerMap.put("maxDomComplete", timeToDomCompleteStat.max());
                innerMap.put("minDomComplete", timeToDomCompleteStat.min());
                innerMap.put("avgDomComplete", timeToDomCompleteStat.avg());
                innerMap.put("devDomComplete", timeToDomCompleteStat.stdDeviation());

                result.add(innerMap);
            }

            if (conf.print_es_query_result) {
                QueryUtil.print(result);
            }

            return result;
        } catch (Exception e) {
            throw new Exception(e);
        }
    }

    private NumberFormat getNumberFormat(){
        NumberFormat nf = NumberFormat.getInstance();
        nf.setGroupingUsed(false);
        nf.setMaximumFractionDigits(3);
        return nf;
    }
}
